---
title: "ChatGPT"
date: 2024-08-25T12:08:58+08:00
modality: ["文本"]
weight : 100
availability: ["海外"]
scenario: ["聊天"]
featured_image: "icons/openai.svg"
description: "获得答案。寻找灵感。更有效率。免费使用。易于尝试。只需询问和ChatGPT可以帮助写作、学习、头脑风暴等等。"
rating: 5
comment: true
website: "https://chat.openai.com/"
---

我们已经训练了一个名为ChatGPT的模型，它以对话的方式进行交互。对话格式使ChatGPT有可能回答后续问题，承认其错误，挑战不正确的前提，并拒绝不适当的请求。ChatGPT是InstructGPT的兄弟模型，它被训练为遵循提示中的指令并提供详细的响应。我们很高兴地介绍ChatGPT以获得用户的反馈并了解其优缺点。在研究预览期间，ChatGPT的使用是免费的。立即在 <https://chatgpt.com/> 尝试。

## Methods

我们使用来自人类反馈的强化学习（RLHF）训练这个模型，使用与InstructGPT相同的方法，但数据采集设置略有不同。我们使用监督微调训练了一个初始模型：人类人工智能培训师提供对话，他们在对话中扮演双方——用户和人工智能助手。我们让培训师访问模型编写的建议，以帮助他们编写响应。我们将这个新的对话数据集与InstructGPT数据集混合在一起，并将其转换为对话格式。为了创建强化学习的奖励模型，我们需要收集比较数据，其中包括两个或多个按质量排序的模型响应。

为了收集这些数据，我们采用了人工智能培训师与聊天机器人的对话。我们随机选择一条模型编写的消息，采样几个替代完成，并让人工智能培训师对它们进行排名。使用这些奖励模型，我们可以使用邻近策略优化来微调模型。

ChatGPT是从GPT-3.5系列的模型中微调的，该系列于2022年初完成了训练。您可以在此处了解有关3.5系列的更多信息（在新窗口中打开）。ChatGPT和GPT-3.5是在Azure AI超级计算基础架构上进行训练的。

## Limitations

ChatGPT有时会写出听起来合理但不正确或荒谬的答案。解决这个问题具有挑战性，因为：（1）在RL训练期间，目前没有事实来源；（2）训练模型更加谨慎会导致它拒绝它能正确回答的问题；（3）监督训练会误导模型，因为理想的答案取决于模型知道什么（在新窗口中打开），而不是人类演示者知道什么。

ChatGPT对输入措辞的调整或多次尝试相同的提示很敏感。例如，给定一个问题的措辞，模型可以声称不知道答案，但只要稍微改变一下措辞，就可以正确回答。

该模型经常过于冗长，过度使用某些短语，例如重申它是由OpenAI训练的语言模型。这些问题源于训练数据中的偏见（培训师更喜欢看起来更全面的更长答案）和众所周知的过度优化问题。

理想情况下，当用户提供模棱两可的查询时，模型会提出澄清问题。相反，我们当前的模型通常会猜测用户的意图。虽然我们已经努力使模型拒绝不适当的请求，但它有时会对有害的指令做出反应或表现出有偏见的行为。

我们正在使用审核API来警告或阻止某些类型的不安全内容，但我们预计它目前会有一些假阴性和阳性。我们渴望收集用户反馈，以帮助我们正在进行的工作来改进这个系统。

## 迭代部署

今天发布的ChatGPT研究版本是OpenAI迭代部署越来越安全和有用的人工智能系统的最新一步。GPT-3和法典等早期模型部署的许多经验教训为本版本提供了安全缓解措施，包括通过使用人类反馈强化学习（RLHF）大幅减少有害和不真实的输出。
